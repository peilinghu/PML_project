Practical Machine Learning Project
=======================================  

### EXECTIVE SUMMARY
The main purpose of this analysis is to build a model to predict the manner the individual subjects performed the exercise based on the HAR dataset at http://groupware.les.inf.puc-rio.br/har.

Four different models were fitted to the training data: classification tree, random forest, support vector machine, and ensemble of the three. Random forest generated the best accuracy on validation data(0.995, same as stacked model), followed by support vector machine(accuracy rate 0.941) then classification tree(accuracy rate 0.499). Stacking the three model doesn't gain accuracy over random forest, so the final model chosen for this analysis is random forest. The expected out of sample error rate is 0.005. The prediction result on the testing data is listed at the bottom of the report.

```{r echo=FALSE, results='hide', warning=FALSE, eval=TRUE, message=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(knitr)
library(AppliedPredictiveModeling)
library(rattle)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(e1071)
library(randomForest)
```

### DATA PROCESSING
Training and testing data were downloaded from source, and loaded into r as data frames. Training data set contains 19622 rows and testing has 20 records. Training data has 160 variables including identities, timestamps, features, and a label(classe). Testing set doesn't contain label (classe). Out of the 160 features in the training data set, 67 have high percentage(98%) of na values, 8 features are id and timestamps related data which don't contribute to the predicting model, and 17 don't provide meaningful information. As a result, 107 features were removed from the training data set. NearZeroVar analysis didn't identify that any variables should be further removed from the data set.  
```{r echo=FALSE, results='hide', warning=FALSE, eval=TRUE}
filepath <- getwd()
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainpath <- file.path(getwd(), "pml-training.csv")
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testpath <- file.path(getwd(), "pml-testing.csv")
```
```{r eval=FALSE, echo=FALSE}
## Read data set into data frame
download.file(trainUrl, trainpath)
download.file(testUrl, testpath)
```
```{r eval=TRUE, echo=FALSE, results='hide'}
# Read file into data frame
dftrain <- read.table(trainpath, sep=",", header=TRUE, na.strings=c("NA",""))
dftest <- read.table(testpath, sep=",", header=TRUE)
set.seed(531)
```

```{r results='hide', warning=FALSE, eval=TRUE, echo=FALSE}
# omit features with high percentage of na data
notna <- rep(0, length(dftrain))
for (i in 1:ncol(dftrain)){
        s <- sum(!is.na(dftrain[,i]))
        notna[i] <- s # get non NA row count of each feature
}
df <- dftrain[,which(notna>=nrow(dftrain)*.5)]

# Remove features which don't provide meaningful information
df <- df[, 8:length(df)]

# Check near zero variables
nzv <- nearZeroVar(df, saveMetrics=TRUE)
print(nzv)

# Split into training and validation set
inTrain <- createDataPartition(df$classe, p = 0.6)[[1]] # split based on classe
training <- df[inTrain,]
val <- df[-inTrain,]
```

The five classes seems to be evenly distributed so there is no concern about bias towards the more common cases for machine learning algorithms.  
```{r eval=TRUE, echo=FALSE}
plot(dftrain$classe, xlab="Class label", ylab="Frequency", main="Histogram of Class", col="blue")
```

### PREDICTIVE MODELING
### Classification Tree
Train a classification tree model with cross-validation, and calculate the accuracy against validation data set. The classification trees model with 5-fold cross-validation produced a 0.499 accuracy rate (out of sample error=0.501).  
```{r echo=TRUE, results='markup', warning=FALSE}
fittree <- train(classe ~., data=training, method=c("rpart"), 
                 trControl=trainControl(method="cv", allowParallel = TRUE))
predtree <- predict(fittree, val)
fancyRpartPlot(fittree$finalModel)
confusionMatrix(predtree, val$classe)
```

### Random Forest
Train with a random forest model and calculate the accuracy against validation data set. The accuracy rate was 0.995(out of sample error=0.005).  
```{r echo=TRUE, results='markup', warning=FALSE}
fitrf <- randomForest(classe ~., data=training)
predrf <- predict(fitrf, val)
confusionMatrix(predrf, val$classe)
```

### Support Vector Machine
Train with support vector machine and calculate the accuracy against validation data set. Support vector machine produced 0.941 accuracy on validation data (out of sample error=0.059).  
```{r echo=TRUE, results='markup', warning=FALSE}
fitsvm <- svm(classe ~., data=training)
predsvm <- predict(fitsvm, val)
confusionMatrix(predsvm, val$classe)
```

### Model Stacking
Stacking the three models together, the resulting accuracy is the same as randome forest(accuracy 0.995).  
```{r warning=FALSE, echo=TRUE, results='markup'}
stack <- data.frame(predtree, predrf, predsvm, classe=val$classe)
combfit <- randomForest(classe ~ ., data=stack)
combpred <- predict(combfit, stack)
confusionMatrix(combpred, val$classe)
```

### PREDICTION
Use random forest model to predict the testing data and the result is listed below.  
```{r}
print(predict(fitrf, dftest))
```